{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b9c9cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import math, os, re, time, random, string\n",
    "import numpy as np, pandas as pd, seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import defaultdict\n",
    "import wordcloud\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from sklearn.metrics import *\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from transformers import TFAutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50c8b18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encode: [101, 4372, 16044, 2033, 999, 102]\n",
      "Decode: [CLS] encode me! [SEP]\n"
     ]
    }
   ],
   "source": [
    "TOKENIZER = AutoTokenizer.from_pretrained(\"bert-large-uncased\")\n",
    "enc = TOKENIZER.encode(\"Encode me!\")\n",
    "dec = TOKENIZER.decode(enc)\n",
    "print(\"Encode: \" + str(enc))\n",
    "print(\"Decode: \" + str(dec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59d64efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1af922fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_encode(data,maximum_len) :\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "  \n",
    "\n",
    "    for i in range(len(data.text)):\n",
    "        encoded = TOKENIZER.encode_plus(data.text[i],\n",
    "                                        add_special_tokens=True,\n",
    "                                        max_length=maximum_len,\n",
    "                                        pad_to_max_length=True,\n",
    "                                        return_attention_mask=True)\n",
    "      \n",
    "        input_ids.append(encoded['input_ids'])\n",
    "        attention_masks.append(encoded['attention_mask'])\n",
    "        \n",
    "    return np.array(input_ids),np.array(attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e085cce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(model_layer, learning_rate, use_meta = T, add_dense =T,\n",
    "               dense_dim = 64, add_dropout = T, dropout = 0.2):\n",
    "    \n",
    "    #define inputs\n",
    "    input_ids = Input(shape=(60,),dtype='int32')\n",
    "    attention_masks = Input(shape=(60,),dtype='int32')\n",
    "    meta_input = Input(shape = (meta_train.shape[1], ))\n",
    "    \n",
    "    #insert BERT layer\n",
    "    transformer_layer = model_layer([input_ids,attention_masks])\n",
    "    \n",
    "    #choose only last hidden-state\n",
    "    output = transformer_layer[1]\n",
    "    \n",
    "    #add meta data\n",
    "    if use_meta:\n",
    "        output = tf.keras.layers.Concatenate()([output, meta_input])\n",
    "    \n",
    "    #add dense relu layer\n",
    "    if add_dense:\n",
    "        print(\"Training with additional dense layer...\")\n",
    "        output = tf.keras.layers.Dense(dense_dim,activation='relu')(output)\n",
    "    \n",
    "    #add dropout\n",
    "    if add_dropout:\n",
    "        print(\"Training with dropout...\")\n",
    "        output = tf.keras.layers.Dropout(dropout)(output)\n",
    "    \n",
    "    #add final node for binary classification\n",
    "    output = tf.keras.layers.Dense(1,activation='sigmoid')(output)\n",
    "    \n",
    "    #assemble and compile\n",
    "    if use_meta:\n",
    "        print(\"Training with meta-data...\")\n",
    "        model = tf.keras.models.Model(inputs = [input_ids,attention_masks, meta_input],outputs = output)\n",
    "    \n",
    "    else:\n",
    "        print(\"Training without meta-data...\")\n",
    "        model = tf.keras.models.Model(inputs = [input_ids,attention_masks],outputs = output)\n",
    "\n",
    "    model.compile(tf.keras.optimizers.Adam(lr=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
